{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%store -r filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = filtered_df.drop(columns=['Outcome'])\n",
    "X.dropna(inplace=True)\n",
    "y = filtered_df['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize and train the models\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"Evaluation for {model_name}:\\n\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Logistic Regression:\n",
      "\n",
      "Accuracy: 0.74\n",
      "Precision: 0.80\n",
      "Recall: 0.46\n",
      "F1 Score: 0.59\n",
      "ROC AUC Score: 0.69\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.81        40\n",
      "           1       0.80      0.46      0.59        26\n",
      "\n",
      "    accuracy                           0.74        66\n",
      "   macro avg       0.76      0.69      0.70        66\n",
      "weighted avg       0.75      0.74      0.72        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Random Forest Classifier:\n",
      "\n",
      "Accuracy: 0.73\n",
      "Precision: 0.79\n",
      "Recall: 0.42\n",
      "F1 Score: 0.55\n",
      "ROC AUC Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.80        40\n",
      "           1       0.79      0.42      0.55        26\n",
      "\n",
      "    accuracy                           0.73        66\n",
      "   macro avg       0.75      0.67      0.68        66\n",
      "weighted avg       0.74      0.73      0.70        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest Classifier\n",
    "evaluate_model(\"Random Forest Classifier\", y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Imbalance**: The classification report for both models shows an imbalanced dataset with a higher number of individuals without diabetes (Class 0) compared to those with diabetes (Class 1). This class imbalance can affect the models' ability to correctly identify individuals with diabetes, as indicated by the lower recall values for Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = filtered_df.drop('Outcome', axis=1)  # Features\n",
    "y = filtered_df['Outcome']  # Binary class column\n",
    "\n",
    "# Create a SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and transform the data to oversample the minority class\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Now, X_resampled and y_resampled contain the oversampled data\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize and train the models\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"Evaluation for {model_name}:\\n\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Logistic Regression:\n",
      "\n",
      "Accuracy: 0.68\n",
      "Precision: 0.68\n",
      "Recall: 0.81\n",
      "F1 Score: 0.74\n",
      "ROC AUC Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.53      0.61        43\n",
      "           1       0.68      0.81      0.74        52\n",
      "\n",
      "    accuracy                           0.68        95\n",
      "   macro avg       0.69      0.67      0.67        95\n",
      "weighted avg       0.69      0.68      0.68        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Random Forest Classifier:\n",
      "\n",
      "Accuracy: 0.86\n",
      "Precision: 0.83\n",
      "Recall: 0.94\n",
      "F1 Score: 0.88\n",
      "ROC AUC Score: 0.85\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.77      0.84        43\n",
      "           1       0.83      0.94      0.88        52\n",
      "\n",
      "    accuracy                           0.86        95\n",
      "   macro avg       0.87      0.85      0.86        95\n",
      "weighted avg       0.87      0.86      0.86        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest Classifier\n",
    "evaluate_model(\"Random Forest Classifier\", y_test, y_pred_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
